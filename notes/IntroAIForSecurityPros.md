# Introduction to Artificial Intelligence for security pros

> Cylance

# Contents:


* **Introduction:** 
* **Clustering:** K-means and DBSCAN Algorithms
* **Classification:**Logistic Regression and Decision Tree Algorithms
* **Probability:**
* **Deep Learning:**


# Introduction

* **Artificial Superintelligence:** Produce computers which are superior to humans in every way
* **Artificial General Intelligence:** Machine as intelligent as a human and equally capable of solving the broad range of problems that require learning and reasoning. (Turing test)
* **Artificial Narrow Intelligence:** Exploits a computer's superior ability to process vast quantities of data and detect parrerns and relationships that would otherwise be difficult or imposible to a human to detect.

## Machine learning in the security domain

The context in wich we have to focus on protecting a specific service is important. 
For example, in a website's domains, the context is connection.
Fortunately, these activities generate a vast ammount of logs by sensors and software.
These logs can provide the contextual data we need to identify and ameliorate threats.
These is the kind of precessing ML excels.


# Clustering Using the K-Means and DBSCAN Algorithms

The purpose of cluster analysis is to segregate data into a set of discrete groups or clusters based on similarities among their keey features or attributes.

A variety of statistical, artificial intelligence, and machine learning techniques can be used to create the clusters.

In the network security domain, cluster analysis tipically proceeds through a well-defined series of data preparation and analysis operations.

## Step 1: Data selection and sampling

Ideally, we want to collect all of the data generated by our network, but sometimes, this is neither possible or practical, so we apply statistical sampling techiques that allow us to create more manageable subset of the data for our analysis.
The sample should reflect the characteristics of the dataset as closeley as possible, or the accuracy of our results may be compromised.

> For example, if we decide to analyze the internet activity for ten different computers, our sample should include representative log entries from all ten systems.

## Step 2: Feature extraction

In this stage, we decide which data elements within our sample should be extracted and subject to analysis. *In ML, this is known as Features*

> In security, the relevant features might include the percentage of ports that are open, closed, or filtered, the application running on each of these ports, and the application version number.

It is good practice to include as many features as needed, excluding the ones you know are irrelevant, because these increase processor time 

## Step 3: Feature Encoding and Vectorization

Most ML algorithms require data to be encoded or represented in some mathematical fashon.
One very common way data can be encoded is by mapping each sample and its set of features to a grid of rows and columns. Once structured in this way, each sample is refered to as a *vector* The entire set of rows and columns is refered to as a *matrix*
The encoding process we use, depends on wether the data representing each feature is *continuous, categorical*, or od some other type.

Continuous data can ocupy any nuber of infinite value within a range of values.
For example, any vlaue between 0 ant 100, for cpu tempertarure.

Categorical data is represented by a small set of permissible values within a much more limited range.
For example, software version or name.
It is inherently useful in defining groups.
For example, OS version or SW name, to group computers with similar characteristics.
These categories, must be encoded as numbers before mathematical analysis.
One example: OS

![OS](./os.png)

It is important to normalize data, so the algorithms give the same weight to diferent values and don't overblow gigher values.

## Step 4: Computation and Graphin

Once we finish maping data, we can import it to a suitable statistical analysis or data mining application. 

## Clustering with K-Means

Clustering analysis introduces the concept of a "Feature space" that can contain thousands of dimensions, one each for every feature in our sample set.
Clustering algorithms assign vectors to particular coordinates in this feature space and then measure the distance between any two vectors to determine weather they are sufficiently similar to be grouped together in the same cluster.

### Caveats

* This version of K-Means works with continuous data only
* The data can be meaningfully grouped into a set of similarly sized clusters

### Clustering session

1. Dataset sampled, vectorized, normalized and imported into scikit-learn
1. Data analyst sets K hyperparameter (Number of clusters to generate)
1. K-means randomly selects three vectors and assigsn coordinate in frature set (Centroids)
1. K-means assigns each vector to it's nearest centroid
1. K-means examnes the average distance from each vector to it's centroid. If the centroid's current location matches this computed average, it remains stationary. Otherwise, the centroid is moved to another coordinate wich matches the computed average
1. K-means repeats step 4 for all of the vectors and reassigns them to clusters based on the new centroid location
1. K-means iterates through step 5-6 until
    1. The centroid stops mooving and it's membership remains fixed: *Convergence*
    1. The algorithm completes the maximum number of iterations specified in advance by the analyst

Once the clustering is completed, the analyst can:

* Evaluate accuracy using a variety of validation techiques
* Convert results into a mathematical model, to assess the cluster membership of new samples
* Analyze the cluster results further using aditional statustical and machine learning techiques

### K-Menas pitfalls and limitations

* The analyst must make an informed guess as to how many clusters should be created
    * Have to repeat the clustering operation until the optimum cluster size has been reached
* The clustering results vary drastically depending on where the first centroids are created
* Euclidean distance breakes down to a measure of similarity in very high dimensional feature spaces. This is known as the "Curse of dimensionality"

## Clustering with DBSCAN

> Density Based Spatial Clustering of Applications with Noise

* DBSCAN identifies clusters by evaluating the density of points within a given region of feature space.
* DBSCAN constructs clusters in regions where vectors are most denesley packed and considers points in sparse regions to be noise.
* DBSCAN discovers for itself how many clusters to create
* Is able to create cluster of virtually any shape and size
* DBSCAN presents analyst with 2 hyperparameters:
    * Epsilon (Eps): Radius of the circular region surrounding each point, to evaluate it's membership
    * Minimum Points (MinPts): Minimum number of points that must appear within an Epsilon neighborhood for the points inside to be included in a cluster.
* DBSCAN performs clustering by examining each point in the dataset and then assigning it to one of three categories:
    * *A core point:* Point that has more than the specified number of MinPts within it's Epsilon neighborhood
    * *A border point:* Point that falls within a core point's neighborhood but has insufficient neighbors of it's own to be a core point
    * *A noise point:* Neither a Core point or a Border point

![DBSCAN Cluster Process](./DBSCAN-corePoint-BorderPoint.png)

### DBSCAN clustering session

1. Dataset is sampled, vectorized, normalized, and then imported into scikit-learn
1. Analyst builds a DBSCAN object and specifies the initial Eps and MinPts values.
1. DBSCAN randomly selects one of the points on the feature space and count the number of points that lie within this point's Eps neighborhood. If this number => than MinPts, then the point is classified as a core point and DBSCAN adds point A and it's neighbors to a cluster
1. DBSCAN moves from Point A to one of it's neighbors, eg Point B, and then classifies it as either a core or border point. If Point B classifies as a core point, then point B and it's neighbors are added to the cluster and assigned the same cluster ID. This process continues until DBSCAN has visited all of the neighbors and detected all of that cluster's core and border points.
1. DBSCAN moves to a point that it has not visited before and repeats steps 3 - 4 until all of the neighbor and noise points have been categorized. When this process concludes, al of the clusters have been identified and issued cluster ID's.

If the results are satisfactory, the clustering session ends. If not, the analyst can:

* Tune the Eps and MinPts hyperparameters
* Change the distance calculation algorithm. DBSCAN supports:
    * Euclidean Distance
    * Manhattan or City Block Distance
    * Cosine Similarity

![Euclidean, Manhattan and Cosine Distances](./Distance-Algorithms.png)

### DBSCAN pitfalls and limitations

* Extremely sensitive to small changes in MinPts and Eps settings.
* < computationally efficient as more dimensions are added
* Performs poorly with datasets that result in regions of varying densities due to the fixed values that must be assigned to MinPts and Eps.

## Assessing Cluster Validity

* Run sample set through an external model and see if the resulting cluster assignments match our own
* Test our results with "Hold out data" (Data from our dataset not used to generate cluster model)
* Use statistical methods. (K-means can use Silhouette Coefficient, wich compares the average distance between points in a cluster to those in other clusters. The lower the result, the better)
* Compare results produced by diferent algorithms or by the same algorithm using diferent hyperparameter settings.

## Cluster Analysis Applied to Real-World Threat Scenarios

### Example with the Panama Papers breach:

If we map our network logs to clusters, including data like upload and download speeds, location of connection, emails processed versus emails sent, we could cluster the data and look for outliers, generated by the DBSCAN cluster algorithm.
This might have been useful to detect strange behaviours and act rapidly upon their discovery. 
This advantage lies in that it only needs data to detect outliers, no IPS / IDS needed or Antivirus

### Clustering Session Utilizing HTTP Log Data (Practical course)

### Clustering takeaways

Clustering provides a mathematically rigorous approach to detecting patterns and relationships among network, application, file, and user data that might be dificult or imposible to secure in any other way.

* Cluster analysis can be applied to virtually any kind of data once the relevant features have been extracted and normalized
* In cluster analysis, similarity between samples and their resulting cluster is determined measuring the distance between vectors.
* K-Means and DBSCAN are easy to use, efficient and broadly applicable. They are also vulnerable to "Curse of dimensionality" and may not be suitable when analyzing extremely high dimensional feature spaces.
* Clustering results must be statistically validated and carefully evaluated with respect to real world security threats.
* Clustering is particularly useful in data exploration and forensic analysis because it allows us to sift through vast ammounts of data to identify outliers.



# Classification Using The Logistic Regression and Decision Tree Algorithms

* **Classification:** The process of assigning an unknown object to a known category in order to make informed decisions
* **Classification in Machine Learning:** A set of computational methods for predicting the likelyhood that a given sample belongs to a predefined class

In **binary classification**, the elements classified, fall either in one class or another. 
(e.g: A email is *spam* or is *not spam*) 
By convention, samples that posess the attributes that we are investigation (email is *spam*), are labeled as belonging to class 1, while that labels that dont posess this attribute, are labeled as belonging to class 0.

In **multiple classification**, a sample can belong to multiple classes at the same time.

In **multinominal classification**, a value is assigned to one class among a value of three or more.

We will consider only binary classification

* **Classifier:** Algorithm used to perform classification.

We will analyze the logic regression and decisions trees. (Binary classification classifiers)

## Supervised Vs. Unsupervised Learning

* **Classification:** Supervised Learning 
    * An analyst builds a model with samples that have already been identified with respect to the property under investigarion.
    * The job of the classifier is to ascertain how the feature attributes of each class can be used to predict the class of new, unlabeled samples.
* **Clustering:** Unsupervised Learning
    * The properties that distinguish one group of samples from another must be discovered.

## Typicall classification session

1. *Training* phase
    1. Analysts construct a model by applying a classifier to a set of training data
    1. The training set consists of two files: A matrix of sample data and a Vector of labels (1 per row in the matrix) 
1. *Validation* phase
    1. Analyst applies the validation data to the model, to assess accuracy
    1. Analyst tweaks hyperparameters, to achieve maximum accuracy
1. *Testing* phase
    1. assess the modal's accuracy with test data withheld from the training and validation phases
    1. If results are satisfactory, the analyst can proceed to *deployment* phase
1. *Deployment* phase
    1. The model is applied to predict the class membership of new, unlabeled data.

## Classification via Logistic Regression (LR)

* **LR** is a linear classifier.
    * Mathematically, LR utilizes straight lines and planes to distinguish vectors belonging to one class or another.
* **Fitting**: The process of carving the feature space into two regions
* **Decision Boundary:** The line or plane that  separates one region from another
* **Solver functions**: Functions used to determine the location of the Decision Boundary
    * We will look at the *liblinear* solver and how it applies the *coordinate descent* method

### The role of regression weights

Then play a central role in determining how much each feature and feature value contributes to a given vector's class membership.
This is achieved by multiplying each feature value by it's coresponding weight:

![Regression Weights](./RegressionWeights.png)

Positive and negative values contribute to class 1 and class 0 respectively.

To predict the class, LR sums all of the products together with a computed *bias* value.
If the aggregate sum is >= 0, LR will predict the sample as belonging to class 1.
If the value is < 0, LR will predict the sample as a class 0 member.

In our example above, LR would sum 12.5, 1050 and -252 for a sum of 815.5.
Since the sum is >= 0, our sample would be predicted to belong in class 1.

Most of the training phase of a LR session is devoted to optimizing the linear regression weights.
The algorithm will eventually find the best distribution of weights, given enough time.

### The role of regularization and penalty parameters

**Regularization:** The use of a penalty parameter C to compress the range of regression weights in much the same way they use normalization to compress feature values.

C controls how large the weighted values can become.
Models with extremely high weight ranges, do a excelent job at predicting the class of training vectors, but produce sub-par results when applied to test data. 
This is called: *over-fit* the data.

**Penalty parameters *L1* and *L2*:** Control wich features are allowed to influence the classifier in computing regression weights. 

*L1* sets a threshold that determines how aggresive LR should be in eliminating features with comparativeley low predictive power.
The higher the weight to *L1*, the more features will be executed.

*L2* minimizes the impact of a group of highly correlated features so that their collective influence does not skew the results.

### Logistic Regression Training Phase

In this phase, the analyst's primary goal is to fit the data by producing an optimized set of regression weights.

*Step 1: Data import and Session Setup*

The analyst imports two files:

1. A matrix of normalized training samples
1. A vector of labels that define each sample's class membership


![Vactor class membership](./VectorClassMemberships.png)

*Step 2: Regularization and Regression Weight Optimization*

An initial set of regression weights are assigned, and the analyst invokes a *likelyhood* function.
This compares the actual number of positive and negative cases to the aggregate quantity predicted using the initial weights.
The resulting score is used to calculate a positive or negative ajustment to each weight's value.
The analyst can control the size of this ajustment on a feature-by-feature basis by utilizing a *learning rate* parameter. 
Over the course of the repeated calculation cycles, the regression weights will gradually and incrementally move closer and closer to their optimal values.
After each optimization, the analyst can experiment with diferent penalty parameter settings and then address toe resulting model.

*Step 3: Assigning Probability Scores to Class Predictions*

LR is intrinsically a method for predicting the probability that a given vector belongs to a particular class. 
LR includes a *logit function* that converts the classification result into a point along a probability curve that ranges from 0 - 1.
The closer the point is to probability score approaching y=1, the stronger the prediction will be that the sample belongs to class 1.
The closer the point is to p=0, the more strongly it will be predicted to belong to class 0.

*Step 4: Exporting the Logistic Regression Model*

The resulting classification model can now be exported and subjected to testing.
Mathematically, the model consists of the bias value along with a vector of regression weights.
Once these have been computed, the coordinates of the decision boundary can be calculated.

In the simplest case of a classification problem with only two features, the equation takes the form `x2 = -(m1/m2) x1 + b` in which *x1* and *x2* are the feature values, *m1* and *m2* are their respective regression weight and *b* is the bias value.

If there are two features, the decision baoundary will comprise a line. 3, a plane, etc...

### Logistic Regression Testing Phase

The analyst assesses the model by exposing it to data it hasn't seen before and then measuring the accuracy of it's predictions.

To validate it's predictions, you can use the *confusion matrix* function.
It examines each sample in turn and then compares it's predicted class membership to it's actual class label. 
Next, it assignes the prediction of that label to pne of 4 categories:

* True Positive: Correctly predicted to belong to class 1
* True Negative: Correctly predicted to belong to class 0
* False Positive: Predicted to be class 1, but is class 0
* False Negative: Predicted to be class 0, but is class 1

**Validation metrics on matrix data**

* **Precision:** Rate at wich a model's positive prediction is a correct one. `P = TP / TP+FP`
* **Recall:** Rate at wich a model correctly classifies a positive case. `R = TP / TP + FP`

**Model's overall accuracy and errors in classifying negative cases**

* **Mean accuracy:** `MA = (TP + TN) / Total samples` 
* **Missclassification Rate:** `MR = 1 - MA`
* **False Positive Rate:** `FPR = FP / Actual Negatives` 
* **Specificity:** `S = TN / Actual Negatives`
* **Prevalence:** `P = Actual Positives / Test Samples`

### Model Evaluation Using Receiver Operating Characteristic Curves

Receiver Operating Characteristic (ROC) Curves provide a convenient and visually intuitive way to access the quality of a model's predictions and to compare the accuracy of one model against another in solving a binary classification problem.

* The more accurate the model is, the closer it's ROC curve will be to the upper left quadrant.
* The `y = x` ROC curve would be a model in wich data is predicted randomly
* A model is predicting a lot of false positives if it's ROC curve is close to the lower right quadrant.

![ROC Space description](./ROCSpace.png)

# Probability
# Deep Learning
